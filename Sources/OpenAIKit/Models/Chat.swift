import Foundation

/// A request to create a chat completion with OpenAI's chat models.
///
/// This struct represents the complete set of parameters you can send to the OpenAI Chat Completions API.
/// At minimum, you need to provide a model identifier and an array of messages representing the conversation.
///
/// ## Example
/// ```swift
/// let request = ChatCompletionRequest(
///     messages: [
///         ChatMessage(role: .system, content: "You are a helpful assistant."),
///         ChatMessage(role: .user, content: "What is the capital of France?")
///     ],
///     model: "gpt-4"
/// )
/// ```
///
/// ## Topics
/// ### Essential Properties
/// - ``messages``
/// - ``model``
///
/// ### Response Control
/// - ``temperature``
/// - ``maxTokens``
/// - ``stop``
/// - ``responseFormat``
///
/// ### Advanced Features
/// - ``tools``
/// - ``functions``
/// - ``stream``
/// - ``audio``
public struct ChatCompletionRequest: Codable, Sendable {
    /// The messages to generate a chat completion for.
    ///
    /// This array represents the conversation history and must contain at least one message.
    /// Messages are processed in the order they appear, with each message having a specific role.
    ///
    /// ## Example
    /// ```swift
    /// let messages = [
    ///     ChatMessage(role: .system, content: "You are a helpful assistant."),
    ///     ChatMessage(role: .user, content: "Hello!"),
    ///     ChatMessage(role: .assistant, content: "Hi! How can I help you today?"),
    ///     ChatMessage(role: .user, content: "What's the weather like?")
    /// ]
    /// ```
    public let messages: [ChatMessage]
    
    /// The ID of the model to use.
    ///
    /// Common model IDs include:
    /// - `"gpt-4"` - Most capable model for complex tasks
    /// - `"gpt-4-turbo"` - Optimized for speed and cost
    /// - `"gpt-3.5-turbo"` - Fast and cost-effective for simpler tasks
    public let model: String
    
    /// Audio input or output configuration.
    ///
    /// Enables the model to process audio inputs or generate audio outputs.
    public let audio: ChatAudio?
    
    /// Number between -2.0 and 2.0.
    ///
    /// Positive values penalize new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim.
    public let frequencyPenalty: Double?
    
    /// Deprecated. Use ``tools`` or ``toolChoice`` instead.
    ///
    /// Controls which (if any) function is called by the model.
    public let functionCall: FunctionCall?
    
    /// Deprecated. Use ``tools`` instead.
    ///
    /// A list of functions the model may generate JSON inputs for.
    public let functions: [Function]?
    
    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Maps token IDs (as strings) to bias values from -100 to 100.
    /// Mathematically, the bias is added to the logits generated by the model prior to sampling.
    public let logitBias: [String: Int]?
    
    /// Whether to return log probabilities of the output tokens.
    ///
    /// If true, returns the log probabilities of each output token returned in the content of message.
    public let logprobs: Bool?
    
    /// The maximum number of completion tokens.
    ///
    /// This is a hard limit on the total number of tokens that can be generated.
    /// Prefer this over ``maxTokens`` for newer models.
    public let maxCompletionTokens: Int?
    
    /// The maximum number of tokens to generate in the completion.
    ///
    /// The token count of your prompt plus `maxTokens` cannot exceed the model's context length.
    /// Use ``maxCompletionTokens`` for newer models.
    public let maxTokens: Int?
    
    /// Developer-defined tags and values for filtering completions.
    ///
    /// Useful for tracking and organizing API usage in production applications.
    public let metadata: [String: String]?
    
    /// The modalities to use for the completion.
    ///
    /// Controls what types of content the model can generate (e.g., "text", "audio").
    public let modalities: [String]?
    
    /// How many chat completion choices to generate for each input message.
    ///
    /// Note that you will be charged based on the number of generated tokens across all choices.
    /// Keep `n` as 1 to minimize costs.
    public let n: Int?
    
    /// Whether to enable parallel function calling during tool use.
    ///
    /// When enabled, the model may call multiple tools in a single response.
    public let parallelToolCalls: Bool?
    
    /// Configuration for predicted outputs to improve latency.
    ///
    /// Allows the model to skip processing of predicted content for faster responses.
    public let prediction: Prediction?
    
    /// Number between -2.0 and 2.0.
    ///
    /// Positive values penalize new tokens based on whether they appear in the text so far,
    /// increasing the model's likelihood to talk about new topics.
    public let presencePenalty: Double?
    
    /// The reasoning effort for the model.
    ///
    /// Controls how much computational effort the model should use for reasoning tasks.
    public let reasoningEffort: String?
    
    /// The format that the model must output.
    ///
    /// Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the model
    /// generates valid JSON. Important: When using JSON mode, instruct the model to produce
    /// JSON via a system or user message.
    public let responseFormat: ResponseFormat?
    
    /// Random seed for deterministic output.
    ///
    /// If specified, the system will make a best effort to sample deterministically,
    /// such that repeated requests with the same seed and parameters should return the same result.
    public let seed: Int?
    
    /// The latency tier to use for processing the request.
    ///
    /// Currently supports "auto" (default) and "default".
    public let serviceTier: String?
    
    /// Up to 4 sequences where the API will stop generating further tokens.
    ///
    /// The returned text will not contain the stop sequence.
    public let stop: StopSequence?
    
    /// Whether to store the completion for model improvement.
    ///
    /// If false, the completion will not be stored or used for model training.
    public let store: Bool?
    
    /// Whether to stream the response.
    ///
    /// If true, partial message deltas will be sent as server-sent events as they become available.
    /// Use ``ChatStreamChunk`` to handle streamed responses.
    public let stream: Bool?
    
    /// Options for streaming responses.
    ///
    /// Only applies when ``stream`` is true.
    public let streamOptions: StreamOptions?
    
    /// The sampling temperature between 0 and 2.
    ///
    /// Higher values like 0.8 will make the output more random, while lower values like 0.2
    /// will make it more focused and deterministic. Generally recommend altering this or
    /// ``topP`` but not both.
    public let temperature: Double?
    
    /// Controls which (if any) tool is called by the model.
    ///
    /// - `none`: The model will not call any tool
    /// - `auto`: The model can pick between generating a message or calling tools
    /// - `required`: The model must call one or more tools
    /// - `function`: Forces the model to call a specific function
    public let toolChoice: ToolChoice?
    
    /// A list of tools the model may call.
    ///
    /// Currently, only functions are supported as a tool. Use this to provide a list of
    /// functions the model may generate JSON inputs for. A max of 128 functions are supported.
    public let tools: [Tool]?
    
    /// The number of most likely tokens to return at each token position.
    ///
    /// Each token includes a log probability. Must be between 0 and 20.
    public let topLogprobs: Int?
    
    /// Nucleus sampling parameter between 0 and 1.
    ///
    /// The model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// Generally recommend altering this or ``temperature`` but not both.
    public let topP: Double?
    
    /// A unique identifier representing your end-user.
    ///
    /// Can help OpenAI monitor and detect abuse.
    public let user: String?
    
    /// Configuration for web search integration.
    ///
    /// Allows the model to search the web for up-to-date information.
    public let webSearchOptions: WebSearchOptions?
    
    /// Creates a new chat completion request.
    ///
    /// - Parameters:
    ///   - messages: The conversation history as an array of ``ChatMessage`` objects
    ///   - model: The model ID to use (e.g., "gpt-4", "gpt-3.5-turbo")
    ///   - audio: Audio configuration for input/output
    ///   - frequencyPenalty: Penalty for token frequency (-2.0 to 2.0)
    ///   - functionCall: Deprecated. Use `tools` instead
    ///   - functions: Deprecated. Use `tools` instead
    ///   - logitBias: Token bias adjustments
    ///   - logprobs: Whether to include log probabilities
    ///   - maxCompletionTokens: Maximum completion tokens (preferred over maxTokens)
    ///   - maxTokens: Maximum tokens to generate
    ///   - metadata: Custom metadata for tracking
    ///   - modalities: Content types the model can generate
    ///   - n: Number of completions to generate
    ///   - parallelToolCalls: Enable parallel tool calling
    ///   - prediction: Predicted output configuration
    ///   - presencePenalty: Penalty for token presence (-2.0 to 2.0)
    ///   - reasoningEffort: Computational effort for reasoning
    ///   - responseFormat: Output format constraints
    ///   - seed: Random seed for deterministic output
    ///   - serviceTier: Processing tier selection
    ///   - stop: Stop sequences for generation
    ///   - store: Whether to store for model improvement
    ///   - stream: Enable streaming responses
    ///   - streamOptions: Configuration for streaming
    ///   - temperature: Sampling temperature (0 to 2)
    ///   - toolChoice: Tool selection strategy
    ///   - tools: Available tools for the model
    ///   - topLogprobs: Number of top log probabilities
    ///   - topP: Nucleus sampling parameter (0 to 1)
    ///   - user: End-user identifier
    ///   - webSearchOptions: Web search configuration
    public init(
        messages: [ChatMessage],
        model: String,
        audio: ChatAudio? = nil,
        frequencyPenalty: Double? = nil,
        functionCall: FunctionCall? = nil,
        functions: [Function]? = nil,
        logitBias: [String: Int]? = nil,
        logprobs: Bool? = nil,
        maxCompletionTokens: Int? = nil,
        maxTokens: Int? = nil,
        metadata: [String: String]? = nil,
        modalities: [String]? = nil,
        n: Int? = nil,
        parallelToolCalls: Bool? = nil,
        prediction: Prediction? = nil,
        presencePenalty: Double? = nil,
        reasoningEffort: String? = nil,
        responseFormat: ResponseFormat? = nil,
        seed: Int? = nil,
        serviceTier: String? = nil,
        stop: StopSequence? = nil,
        store: Bool? = nil,
        stream: Bool? = nil,
        streamOptions: StreamOptions? = nil,
        temperature: Double? = nil,
        toolChoice: ToolChoice? = nil,
        tools: [Tool]? = nil,
        topLogprobs: Int? = nil,
        topP: Double? = nil,
        user: String? = nil,
        webSearchOptions: WebSearchOptions? = nil
    ) {
        self.messages = messages
        self.model = model
        self.audio = audio
        self.frequencyPenalty = frequencyPenalty
        self.functionCall = functionCall
        self.functions = functions
        self.logitBias = logitBias
        self.logprobs = logprobs
        self.maxCompletionTokens = maxCompletionTokens
        self.maxTokens = maxTokens
        self.metadata = metadata
        self.modalities = modalities
        self.n = n
        self.parallelToolCalls = parallelToolCalls
        self.prediction = prediction
        self.presencePenalty = presencePenalty
        self.reasoningEffort = reasoningEffort
        self.responseFormat = responseFormat
        self.seed = seed
        self.serviceTier = serviceTier
        self.stop = stop
        self.store = store
        self.stream = stream
        self.streamOptions = streamOptions
        self.temperature = temperature
        self.toolChoice = toolChoice
        self.tools = tools
        self.topLogprobs = topLogprobs
        self.topP = topP
        self.user = user
        self.webSearchOptions = webSearchOptions
    }
}

/// A message in a chat conversation.
///
/// Chat messages represent the building blocks of a conversation with the model. Each message
/// has a role that determines who is speaking and content that contains what was said.
///
/// ## Example
/// ```swift
/// // Simple text message
/// let userMessage = ChatMessage(role: .user, content: "Hello!")
/// 
/// // Message with image
/// let imageMessage = ChatMessage(
///     role: .user,
///     content: .parts([
///         MessagePart(type: .text, text: "What's in this image?"),
///         MessagePart(type: .imageUrl, imageUrl: ImageURL(url: "https://example.com/image.jpg"))
///     ])
/// )
/// 
/// // Tool response message
/// let toolMessage = ChatMessage(
///     role: .tool,
///     content: "The weather is sunny and 72°F",
///     toolCallId: "call_123"
/// )
/// ```
///
/// ## Topics
/// ### Message Components
/// - ``role``
/// - ``content``
/// - ``name``
///
/// ### Tool Integration
/// - ``toolCalls``
/// - ``toolCallId``
public struct ChatMessage: Codable, Sendable {
    /// The role of the message author.
    ///
    /// Determines who is speaking in the conversation. Each role has specific behaviors:
    /// - `.system`: Sets the assistant's behavior
    /// - `.user`: Represents the human user
    /// - `.assistant`: The AI model's responses
    /// - `.tool`: Results from tool/function calls
    public let role: ChatRole
    
    /// The content of the message.
    ///
    /// Can be either a simple string or an array of content parts for multimodal messages.
    /// Note: When the assistant makes function/tool calls, this may be nil.
    public let content: MessageContent?
    
    /// An optional name for the message author.
    ///
    /// Useful for distinguishing between multiple users or assistants in a conversation.
    public let name: String?
    
    /// Tool calls made by the assistant.
    ///
    /// When the assistant decides to use tools, this array contains the details of which
    /// tools to call and with what arguments.
    public let toolCalls: [ToolCall]?
    
    /// The ID of the tool call this message is responding to.
    ///
    /// Required when `role` is `.tool`, this links the tool's response back to the
    /// assistant's original tool call request.
    public let toolCallId: String?
    
    /// Creates a new chat message with full configuration options.
    ///
    /// - Parameters:
    ///   - role: The role of the message author
    ///   - content: The message content (text or multimodal)
    ///   - name: Optional name for the author
    ///   - toolCalls: Tool calls made by the assistant
    ///   - toolCallId: ID when responding to a tool call
    public init(
        role: ChatRole,
        content: MessageContent?,
        name: String? = nil,
        toolCalls: [ToolCall]? = nil,
        toolCallId: String? = nil
    ) {
        self.role = role
        self.content = content
        self.name = name
        self.toolCalls = toolCalls
        self.toolCallId = toolCallId
    }
    
    /// Creates a new chat message with simple text content.
    ///
    /// This convenience initializer is perfect for basic text-only messages.
    ///
    /// - Parameters:
    ///   - role: The role of the message author
    ///   - content: The text content of the message
    ///
    /// ## Example
    /// ```swift
    /// let message = ChatMessage(role: .user, content: "What is the capital of France?")
    /// ```
    public init(role: ChatRole, content: String) {
        self.init(role: role, content: .string(content))
    }
    
    /// Creates a new chat message without content.
    ///
    /// This is useful for function/tool call responses where content may be nil.
    ///
    /// - Parameters:
    ///   - role: The role of the message author
    ///   - name: Optional name for the author
    ///   - toolCalls: Tool calls made by the assistant
    ///   - toolCallId: ID when responding to a tool call
    public init(
        role: ChatRole,
        name: String? = nil,
        toolCalls: [ToolCall]? = nil,
        toolCallId: String? = nil
    ) {
        self.init(role: role, content: nil, name: name, toolCalls: toolCalls, toolCallId: toolCallId)
    }
}

/// The role of a message author in a chat conversation.
///
/// Each role has specific purposes and behaviors in the conversation flow:
///
/// ## Usage
/// ```swift
/// let messages = [
///     ChatMessage(role: .system, content: "You are a helpful coding assistant."),
///     ChatMessage(role: .user, content: "How do I sort an array in Swift?"),
///     ChatMessage(role: .assistant, content: "You can use the `sorted()` method..."),
///     ChatMessage(role: .tool, content: "Code executed successfully.")
/// ]
/// ```
public enum ChatRole: String, Codable, Sendable {
    /// System message that sets the assistant's behavior.
    ///
    /// System messages are typically placed at the beginning of a conversation to provide
    /// instructions, set the tone, or define the assistant's persona.
    case system
    
    /// Message from the human user.
    ///
    /// User messages contain questions, requests, or responses from the person interacting
    /// with the assistant.
    case user
    
    /// Message from the AI assistant.
    ///
    /// Assistant messages contain the model's responses, including text replies and tool calls.
    case assistant
    
    /// Message containing results from a tool or function call.
    ///
    /// Tool messages provide the output from functions called by the assistant, allowing
    /// the model to use external capabilities and information.
    case tool
}

/// The content of a chat message.
///
/// Message content can be either simple text or a collection of multimodal parts including
/// text and images. This flexibility allows for rich interactions with vision-capable models.
///
/// ## Examples
/// ```swift
/// // Simple text content
/// let textContent: MessageContent = .string("Hello, how can I help?")
/// 
/// // Multimodal content with text and image
/// let multimodalContent: MessageContent = .parts([
///     MessagePart(type: .text, text: "What's in this image?"),
///     MessagePart(type: .imageUrl, imageUrl: ImageURL(url: "https://example.com/photo.jpg"))
/// ])
/// ```
public enum MessageContent: Codable, Sendable {
    /// Simple text content.
    ///
    /// Use this for standard text-only messages.
    case string(String)
    
    /// Multimodal content with multiple parts.
    ///
    /// Use this for messages that combine text with images or other content types.
    /// Each part is a ``MessagePart`` that specifies its type and content.
    case parts([MessagePart])
    
    public init(from decoder: Decoder) throws {
        let container = try decoder.singleValueContainer()
        
        // First check if the value is null
        if container.decodeNil() {
            // If we have a null, we need to throw since MessageContent itself can't represent null
            // The parent ChatMessage should handle this as an optional
            throw DecodingError.typeMismatch(
                MessageContent.self,
                DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "MessageContent cannot be null")
            )
        }
        
        if let string = try? container.decode(String.self) {
            self = .string(string)
        } else if let parts = try? container.decode([MessagePart].self) {
            self = .parts(parts)
        } else {
            throw DecodingError.typeMismatch(
                MessageContent.self,
                DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Expected String or [MessagePart]")
            )
        }
    }
    
    public func encode(to encoder: Encoder) throws {
        var container = encoder.singleValueContainer()
        switch self {
        case .string(let string):
            try container.encode(string)
        case .parts(let parts):
            try container.encode(parts)
        }
    }
}

/// A single part of a multimodal message.
///
/// Message parts allow you to combine different types of content (text, images, etc.) within
/// a single message. This is particularly useful for vision-capable models.
///
/// ## Example
/// ```swift
/// let textPart = MessagePart(type: .text, text: "Describe this image:")
/// let imagePart = MessagePart(
///     type: .imageUrl,
///     imageUrl: ImageURL(url: "data:image/jpeg;base64,...")
/// )
/// ```
public struct MessagePart: Codable, Sendable {
    /// The type of content in this part.
    public let type: MessagePartType
    
    /// Text content when `type` is `.text`.
    public let text: String?
    
    /// Image URL information when `type` is `.imageUrl`.
    public let imageUrl: ImageURL?
    
    /// Creates a new message part.
    ///
    /// - Parameters:
    ///   - type: The type of content
    ///   - text: Text content (required when type is `.text`)
    ///   - imageUrl: Image URL information (required when type is `.imageUrl`)
    public init(type: MessagePartType, text: String? = nil, imageUrl: ImageURL? = nil) {
        self.type = type
        self.text = text
        self.imageUrl = imageUrl
    }
}

/// The type of content in a message part.
public enum MessagePartType: String, Codable, Sendable {
    /// Text content.
    case text
    
    /// Image content provided via URL.
    ///
    /// The URL can be a web URL or a base64-encoded data URL.
    case imageUrl = "image_url"
}

/// Image information for multimodal messages.
///
/// Supports both web URLs and base64-encoded data URLs. The detail level controls
/// how the model processes the image, affecting both quality and token usage.
///
/// ## Example
/// ```swift
/// // Web URL
/// let webImage = ImageURL(url: "https://example.com/image.jpg", detail: .high)
/// 
/// // Base64 data URL
/// let dataImage = ImageURL(
///     url: "data:image/jpeg;base64,/9j/4AAQSkZJRg...",
///     detail: .low
/// )
/// ```
public struct ImageURL: Codable, Sendable {
    /// The URL of the image.
    ///
    /// Can be either:
    /// - A standard web URL (https://...)
    /// - A base64-encoded data URL (data:image/jpeg;base64,...)
    public let url: String
    
    /// The level of detail for image processing.
    ///
    /// Controls the resolution at which the model processes the image.
    /// Higher detail levels provide better quality but use more tokens.
    public let detail: ImageDetail?
    
    /// Creates a new image URL reference.
    ///
    /// - Parameters:
    ///   - url: The image URL (web or data URL)
    ///   - detail: Processing detail level (defaults to model's choice)
    public init(url: String, detail: ImageDetail? = nil) {
        self.url = url
        self.detail = detail
    }
}

/// The level of detail for image processing.
///
/// Controls how the model processes images, balancing between quality and token usage.
public enum ImageDetail: String, Codable, Sendable {
    /// Let the model choose the appropriate detail level.
    case auto
    
    /// Low detail mode.
    ///
    /// Faster and uses fewer tokens, but may miss fine details in the image.
    case low
    
    /// High detail mode.
    ///
    /// Provides the best quality but uses more tokens and processing time.
    case high
}

/// The response from a chat completion request.
///
/// Contains the model's response along with metadata about the generation process.
/// When `n` > 1 in the request, multiple choices will be returned.
///
/// ## Example
/// ```swift
/// // Handling a chat completion response
/// let response: ChatCompletionResponse = try await openAI.chat(request)
/// 
/// if let firstChoice = response.choices.first {
///     print("Assistant: \(firstChoice.message.content)")
///     print("Tokens used: \(response.usage?.totalTokens ?? 0)")
/// }
/// ```
///
/// ## Topics
/// ### Response Data
/// - ``id``
/// - ``model``
/// - ``choices``
///
/// ### Metadata
/// - ``usage``
/// - ``systemFingerprint``
/// - ``created``
public struct ChatCompletionResponse: Codable, Sendable {
    /// Unique identifier for this completion.
    public let id: String
    
    /// Object type, always "chat.completion".
    public let object: String
    
    /// Unix timestamp (in seconds) when the completion was created.
    public let created: Int
    
    /// The model used for this completion.
    ///
    /// May differ from the requested model if a fallback occurred.
    public let model: String
    
    /// The list of completion choices.
    ///
    /// Contains one or more responses based on the `n` parameter in the request.
    /// Each choice represents a different completion for the same prompt.
    public let choices: [ChatChoice]
    
    /// Token usage statistics for this completion.
    ///
    /// Provides detailed information about token consumption for billing
    /// and optimization purposes.
    public let usage: Usage?
    
    /// System fingerprint for this completion.
    ///
    /// Represents the backend configuration used. Useful for debugging
    /// and ensuring reproducibility.
    public let systemFingerprint: String?
}

/// A single completion choice from the model.
///
/// Each choice represents one possible response to the prompt. When using `n` > 1
/// in the request, multiple choices provide alternative completions.
///
/// ## Example
/// ```swift
/// for choice in response.choices {
///     print("Choice \(choice.index): \(choice.message.content)")
///     
///     if let reason = choice.finishReason {
///         print("Stopped because: \(reason)")
///     }
/// }
/// ```
public struct ChatChoice: Codable, Sendable {
    /// The index of this choice in the list.
    ///
    /// Zero-based index corresponding to the position in the choices array.
    public let index: Int
    
    /// The message generated by the model.
    ///
    /// Contains the assistant's response, including any tool calls if applicable.
    public let message: ChatMessage
    
    /// The reason the model stopped generating.
    ///
    /// Helps understand whether the completion ended naturally or due to constraints.
    public let finishReason: FinishReason?
    
    /// Log probability information.
    ///
    /// Only present when `logprobs` is true in the request.
    public let logprobs: Logprobs?
}

/// The reason why the model stopped generating tokens.
///
/// Understanding finish reasons helps handle different completion scenarios appropriately.
public enum FinishReason: String, Codable, Sendable {
    /// Natural end of message.
    ///
    /// The model completed its response naturally.
    case stop
    
    /// Maximum token limit reached.
    ///
    /// The response was cut off due to reaching `maxTokens` or model limits.
    case length
    
    /// Model decided to call tools.
    ///
    /// The response includes tool calls that need to be executed.
    case toolCalls = "tool_calls"
    
    /// Content was filtered.
    ///
    /// The response was blocked or modified due to content filtering policies.
    case contentFilter = "content_filter"
}

/// Token usage statistics for a completion.
///
/// Provides detailed breakdown of token consumption, useful for cost tracking
/// and optimization. All token counts include both text and special tokens.
///
/// ## Example
/// ```swift
/// if let usage = response.usage {
///     print("Prompt tokens: \(usage.promptTokens)")
///     print("Completion tokens: \(usage.completionTokens)")
///     print("Total cost: \(usage.totalTokens) tokens")
/// }
/// ```
public struct Usage: Codable, Sendable {
    /// Number of tokens in the prompt.
    ///
    /// Includes all messages, system prompts, and formatting tokens.
    public let promptTokens: Int
    
    /// Number of tokens in the completion.
    ///
    /// The tokens generated by the model in its response.
    public let completionTokens: Int
    
    /// Total tokens used (prompt + completion).
    ///
    /// The sum of `promptTokens` and `completionTokens`.
    public let totalTokens: Int
    
    /// Detailed breakdown of completion tokens.
    ///
    /// Provides granular information about different types of completion tokens.
    public let completionTokensDetails: CompletionTokensDetails?
    
    /// Detailed breakdown of prompt tokens.
    ///
    /// Provides information about cached tokens and other optimizations.
    public let promptTokensDetails: PromptTokensDetails?
}

/// Detailed breakdown of completion token usage.
///
/// Provides granular information about different types of tokens used in the completion,
/// particularly useful for models with special capabilities like reasoning or audio.
public struct CompletionTokensDetails: Codable, Sendable {
    /// Tokens used for reasoning steps.
    ///
    /// Present in models that show their reasoning process.
    public let reasoningTokens: Int?
    
    /// Tokens used for audio generation.
    ///
    /// Present when generating audio outputs.
    public let audioTokens: Int?
    
    /// Accepted tokens from prediction.
    ///
    /// Tokens that matched the predicted output.
    public let acceptedPredictionTokens: Int?
    
    /// Rejected tokens from prediction.
    ///
    /// Tokens that didn't match the predicted output.
    public let rejectedPredictionTokens: Int?
}

/// Detailed breakdown of prompt token usage.
///
/// Provides information about optimizations applied to the prompt tokens.
public struct PromptTokensDetails: Codable, Sendable {
    /// Tokens used for audio input.
    ///
    /// Present when processing audio inputs.
    public let audioTokens: Int?
    
    /// Tokens retrieved from cache.
    ///
    /// Cached tokens reduce processing time and cost.
    public let cachedTokens: Int?
}

/// A chunk of data from a streaming chat completion.
///
/// When streaming is enabled, the response is sent as a series of chunks via
/// server-sent events. Each chunk contains incremental updates to the completion.
///
/// ## Example
/// ```swift
/// // Processing streaming responses
/// for try await chunk in stream {
///     if let content = chunk.choices.first?.delta.content {
///         print(content, terminator: "")
///     }
/// }
/// ```
///
/// ## Topics
/// ### Stream Data
/// - ``choices``
/// - ``ChatStreamChoice``
/// - ``ChatDelta``
///
/// ### Metadata
/// - ``id``
/// - ``model``
/// - ``usage``
public struct ChatStreamChunk: Codable, Sendable {
    /// Unique identifier for this completion stream.
    public let id: String
    
    /// Object type, always "chat.completion.chunk".
    public let object: String
    
    /// Unix timestamp when the chunk was created.
    public let created: Int
    
    /// The model used for this completion.
    public let model: String
    
    /// Incremental choice updates.
    ///
    /// Each choice contains a delta with partial content updates.
    public let choices: [ChatStreamChoice]
    
    /// Token usage statistics.
    ///
    /// Only included in the final chunk when `streamOptions.includeUsage` is true.
    public let usage: Usage?
    
    /// System fingerprint for this completion.
    public let systemFingerprint: String?
}

/// A streaming choice containing incremental updates.
///
/// Represents partial updates to a completion choice. The delta contains
/// only the new content since the last chunk.
public struct ChatStreamChoice: Codable, Sendable {
    /// The index of this choice.
    public let index: Int
    
    /// Incremental content update.
    ///
    /// Contains only the new content added since the last chunk.
    public let delta: ChatDelta
    
    /// The reason the model stopped generating.
    ///
    /// Only present in the final chunk for this choice.
    public let finishReason: FinishReason?
    
    /// Log probability information for this chunk.
    public let logprobs: Logprobs?
}

/// Incremental updates in a streaming response.
///
/// Contains partial content that should be appended to previously received content
/// to build the complete response.
///
/// ## Example
/// ```swift
/// var fullContent = ""
/// var toolCalls = [ToolCall]()
/// 
/// for chunk in chunks {
///     if let content = chunk.delta.content {
///         fullContent += content
///     }
///     if let calls = chunk.delta.toolCalls {
///         // Merge or append tool calls
///     }
/// }
/// ```
public struct ChatDelta: Codable, Sendable {
    /// Role of the message author.
    ///
    /// Usually only present in the first chunk.
    public let role: ChatRole?
    
    /// Incremental text content.
    ///
    /// Append this to previously received content.
    public let content: String?
    
    /// Tool calls being constructed.
    ///
    /// May be sent across multiple chunks that need to be merged.
    public let toolCalls: [ToolCall]?
}

/// Stop sequences for controlling generation.
///
/// The model will stop generating further tokens when it encounters any of these sequences.
/// The stop sequence itself is not included in the response.
///
/// ## Example
/// ```swift
/// // Single stop sequence
/// let request1 = ChatCompletionRequest(
///     messages: messages,
///     model: "gpt-4",
///     stop: .string("\n")
/// )
/// 
/// // Multiple stop sequences
/// let request2 = ChatCompletionRequest(
///     messages: messages,
///     model: "gpt-4",
///     stop: .array(["\n", "END", "STOP"])
/// )
/// ```
public enum StopSequence: Codable, Sendable {
    /// A single stop sequence.
    case string(String)
    
    /// Multiple stop sequences (up to 4).
    case array([String])
    
    public init(from decoder: Decoder) throws {
        let container = try decoder.singleValueContainer()
        if let string = try? container.decode(String.self) {
            self = .string(string)
        } else if let array = try? container.decode([String].self) {
            self = .array(array)
        } else {
            throw DecodingError.typeMismatch(
                StopSequence.self,
                DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Expected String or [String]")
            )
        }
    }
    
    public func encode(to encoder: Encoder) throws {
        var container = encoder.singleValueContainer()
        switch self {
        case .string(let string):
            try container.encode(string)
        case .array(let array):
            try container.encode(array)
        }
    }
}

/// A tool that the model can use.
///
/// Tools extend the model's capabilities by allowing it to call functions,
/// run code, or search files. Currently, function calling is the most common tool type.
///
/// ## Example
/// ```swift
/// let weatherTool = Tool(
///     type: .function,
///     function: Function(
///         name: "get_weather",
///         description: "Get the current weather in a location",
///         parameters: [
///             "type": "object",
///             "properties": [
///                 "location": ["type": "string", "description": "The city and state"],
///                 "unit": ["type": "string", "enum": ["celsius", "fahrenheit"]]
///             ],
///             "required": ["location"]
///         ]
///     )
/// )
/// ```
///
/// ## Topics
/// ### Tool Configuration
/// - ``type``
/// - ``function``
/// - ``ToolType``
public struct Tool: Codable, Sendable {
    /// The type of tool.
    public let type: ToolType
    
    /// Function definition when `type` is `.function`.
    ///
    /// Contains the function's name, description, and parameter schema.
    public let function: Function?
    
    /// Creates a new tool definition.
    ///
    /// - Parameters:
    ///   - type: The type of tool
    ///   - function: Function definition (required when type is `.function`)
    public init(type: ToolType, function: Function? = nil) {
        self.type = type
        self.function = function
    }
}

/// The type of tool available to the model.
public enum ToolType: String, Codable, Sendable {
    /// Function calling tool.
    ///
    /// Allows the model to call custom functions with structured inputs.
    case function
    
    /// Code interpreter tool.
    ///
    /// Enables the model to write and execute Python code.
    case codeInterpreter = "code_interpreter"
    
    /// File search tool.
    ///
    /// Allows the model to search through uploaded files.
    case fileSearch = "file_search"
    
    /// Web search preview tool.
    ///
    /// Enables the model to search the web for information.
    case webSearchPreview = "web_search_preview"
    
    /// MCP (Model Context Protocol) tool.
    ///
    /// Allows the model to interact with MCP-compliant tools.
    case mcp
}

/// A function that the model can call.
///
/// Functions allow the model to interact with external systems or perform computations
/// beyond its built-in capabilities. The model will generate structured JSON arguments
/// that match the provided parameter schema.
///
/// ## Example
/// ```swift
/// let function = Function(
///     name: "calculate_discount",
///     description: "Calculate the discount price for a product",
///     parameters: [
///         "type": "object",
///         "properties": [
///             "original_price": [
///                 "type": "number",
///                 "description": "The original price of the product"
///             ],
///             "discount_percentage": [
///                 "type": "number",
///                 "description": "The discount percentage (0-100)"
///             ]
///         ],
///         "required": ["original_price", "discount_percentage"]
///     ]
/// )
/// ```
///
/// ## Topics
/// ### Function Definition
/// - ``name``
/// - ``description``
/// - ``parameters``
public struct Function: Codable, Sendable {
    /// The name of the function.
    ///
    /// Must be a valid identifier (letters, numbers, underscores).
    /// This is how the model will reference the function in its calls.
    public let name: String
    
    /// A description of what the function does.
    ///
    /// Used by the model to understand when and how to use this function.
    /// Be clear and specific about the function's purpose and behavior.
    public let description: String?
    
    /// The parameters the function accepts, described as a JSON Schema.
    ///
    /// Defines the structure and types of arguments the model should provide.
    /// Follow JSON Schema specification for parameter definitions.
    public let parameters: JSONValue?
    
    /// Creates a function with pre-encoded JSON Schema parameters.
    ///
    /// - Parameters:
    ///   - name: The function name
    ///   - description: What the function does
    ///   - parameters: JSON Schema as ``JSONValue``
    public init(name: String, description: String? = nil, parameters: JSONValue? = nil) {
        self.name = name
        self.description = description
        self.parameters = parameters
    }
    
    /// Creates a function with dictionary-based parameters.
    ///
    /// This convenience initializer automatically converts Swift dictionaries
    /// to the required ``JSONValue`` format.
    ///
    /// - Parameters:
    ///   - name: The function name
    ///   - description: What the function does
    ///   - parameters: JSON Schema as a dictionary
    public init(name: String, description: String? = nil, parameters: [String: Any]? = nil) {
        self.name = name
        self.description = description
        if let params = parameters {
            self.parameters = Self.convertToJSONValue(params)
        } else {
            self.parameters = nil
        }
    }
    
    private static func convertToJSONValue(_ dict: [String: Any]) -> JSONValue {
        var result = [String: JSONValue]()
        for (key, value) in dict {
            if let string = value as? String {
                result[key] = .string(string)
            } else if let int = value as? Int {
                result[key] = .int(int)
            } else if let double = value as? Double {
                result[key] = .double(double)
            } else if let bool = value as? Bool {
                result[key] = .bool(bool)
            } else if let dict = value as? [String: Any] {
                result[key] = convertToJSONValue(dict)
            } else if let array = value as? [Any] {
                result[key] = convertToJSONValue(array)
            }
        }
        return .object(result)
    }
    
    private static func convertToJSONValue(_ array: [Any]) -> JSONValue {
        var result = [JSONValue]()
        for value in array {
            if let string = value as? String {
                result.append(.string(string))
            } else if let int = value as? Int {
                result.append(.int(int))
            } else if let double = value as? Double {
                result.append(.double(double))
            } else if let bool = value as? Bool {
                result.append(.bool(bool))
            } else if let dict = value as? [String: Any] {
                result.append(convertToJSONValue(dict))
            } else if let array = value as? [Any] {
                result.append(convertToJSONValue(array))
            }
        }
        return .array(result)
    }
}

/// A tool call made by the model.
///
/// When the model decides to use a tool, it generates a tool call with a unique ID
/// and the necessary arguments. Your application should execute the tool and return
/// the results using a tool message.
///
/// ## Example
/// ```swift
/// if let toolCalls = message.toolCalls {
///     for toolCall in toolCalls {
///         if toolCall.type == .function,
///            let functionCall = toolCall.function {
///             // Parse arguments and execute function
///             let result = executeFunction(
///                 name: functionCall.name,
///                 arguments: functionCall.arguments
///             )
///             
///             // Return result as tool message
///             let toolMessage = ChatMessage(
///                 role: .tool,
///                 content: result,
///                 toolCallId: toolCall.id
///             )
///         }
///     }
/// }
/// ```
public struct ToolCall: Codable, Sendable {
    /// Unique identifier for this tool call.
    ///
    /// Use this ID when sending the tool's response back to the model.
    public let id: String
    
    /// The type of tool being called.
    public let type: ToolType
    
    /// Function call details when `type` is `.function`.
    public let function: FunctionCall?
    
    /// Creates a new tool call.
    ///
    /// - Parameters:
    ///   - id: Unique identifier for the call
    ///   - type: Type of tool being called
    ///   - function: Function details (required when type is `.function`)
    public init(id: String, type: ToolType, function: FunctionCall? = nil) {
        self.id = id
        self.type = type
        self.function = function
    }
}

/// Details of a function call.
///
/// Contains the function name and JSON-encoded arguments that the model
/// has generated based on the function's parameter schema.
///
/// ## Example
/// ```swift
/// // Parse and use function call arguments
/// if let data = functionCall.arguments.data(using: .utf8),
///    let args = try? JSONDecoder().decode(WeatherArgs.self, from: data) {
///     let weather = getWeather(location: args.location, unit: args.unit)
///     // Return weather data as tool response
/// }
/// ```
public struct FunctionCall: Codable, Sendable {
    /// The name of the function to call.
    ///
    /// Matches one of the function names provided in the tools array.
    public let name: String
    
    /// JSON-encoded arguments for the function.
    ///
    /// The arguments match the schema defined in the function's parameters.
    /// Parse this JSON string to extract the actual argument values.
    public let arguments: String
    
    /// Creates a new function call.
    ///
    /// - Parameters:
    ///   - name: The function name
    ///   - arguments: JSON-encoded arguments string
    public init(name: String, arguments: String) {
        self.name = name
        self.arguments = arguments
    }
}

/// Controls which tools the model can use.
///
/// Provides fine-grained control over tool usage, from disabling all tools
/// to requiring specific function calls.
///
/// ## Example
/// ```swift
/// // Let the model decide
/// let request1 = ChatCompletionRequest(
///     messages: messages,
///     model: "gpt-4",
///     tools: tools,
///     toolChoice: .auto
/// )
/// 
/// // Force specific function
/// let request2 = ChatCompletionRequest(
///     messages: messages,
///     model: "gpt-4",
///     tools: tools,
///     toolChoice: .function(name: "get_weather")
/// )
/// ```
public enum ToolChoice: Codable, Sendable {
    /// The model will not call any tools.
    case none
    
    /// The model can choose to call tools or respond with text.
    ///
    /// This is the default behavior when tools are provided.
    case auto
    
    /// The model must call at least one tool.
    ///
    /// Useful when you need the model to use tools rather than just respond.
    case required
    
    /// The model must call the specified function.
    ///
    /// Forces the model to use a specific function by name.
    case function(name: String)
    
    public init(from decoder: Decoder) throws {
        let container = try decoder.singleValueContainer()
        if let string = try? container.decode(String.self) {
            switch string {
            case "none": self = .none
            case "auto": self = .auto
            case "required": self = .required
            default:
                throw DecodingError.dataCorruptedError(in: container, debugDescription: "Unknown tool choice: \(string)")
            }
        } else if let dict = try? container.decode(JSONValue.self),
                  let type = dict.type?.stringValue,
                  type == "function",
                  let name = dict.function?.name?.stringValue {
            self = .function(name: name)
        } else {
            throw DecodingError.typeMismatch(ToolChoice.self, DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Expected String or specific object structure"))
        }
    }
    
    public func encode(to encoder: Encoder) throws {
        var container = encoder.singleValueContainer()
        switch self {
        case .none:
            try container.encode("none")
        case .auto:
            try container.encode("auto")
        case .required:
            try container.encode("required")
        case .function(let name):
            let functionDict: JSONValue = [
                "type": "function",
                "function": ["name": .string(name)]
            ]
            try container.encode(functionDict)
        }
    }
}

/// Response format configuration.
///
/// Controls the format of the model's output, ensuring it returns text,
/// JSON objects, or conforms to a specific JSON schema.
///
/// ## Example
/// ```swift
/// // JSON mode
/// let jsonFormat = ResponseFormat(type: .jsonObject)
/// 
/// // Structured output with schema
/// let structuredFormat = ResponseFormat(
///     type: .jsonSchema,
///     jsonSchema: JSONSchema(
///         name: "recipe",
///         schema: [
///             "type": "object",
///             "properties": [
///                 "name": ["type": "string"],
///                 "ingredients": [
///                     "type": "array",
///                     "items": ["type": "string"]
///                 ],
///                 "instructions": ["type": "string"]
///             ],
///             "required": ["name", "ingredients", "instructions"]
///         ]
///     )
/// )
/// ```
public struct ResponseFormat: Codable, Sendable {
    /// The type of response format.
    public let type: ResponseFormatType
    
    /// JSON schema when `type` is `.jsonSchema`.
    ///
    /// Defines the exact structure the response must follow.
    public let jsonSchema: JSONSchema?
    
    /// Creates a new response format configuration.
    ///
    /// - Parameters:
    ///   - type: The format type
    ///   - jsonSchema: Schema definition (required when type is `.jsonSchema`)
    public init(type: ResponseFormatType, jsonSchema: JSONSchema? = nil) {
        self.type = type
        self.jsonSchema = jsonSchema
    }
}

/// The type of response format.
public enum ResponseFormatType: String, Codable, Sendable {
    /// Standard text response.
    ///
    /// The default format for model responses.
    case text
    
    /// JSON object response.
    ///
    /// Ensures the model returns valid JSON. Remember to instruct the model
    /// to produce JSON in your prompt.
    case jsonObject = "json_object"
    
    /// Structured JSON following a schema.
    ///
    /// The model's response will conform to the provided JSON schema.
    case jsonSchema = "json_schema"
}

/// JSON schema definition for structured outputs.
///
/// Defines the exact structure that the model's JSON response must follow.
/// This ensures type-safe, predictable outputs that can be reliably parsed.
///
/// ## Example
/// ```swift
/// let schema = JSONSchema(
///     name: "math_response",
///     schema: [
///         "type": "object",
///         "properties": [
///             "steps": [
///                 "type": "array",
///                 "items": [
///                     "type": "object",
///                     "properties": [
///                         "explanation": ["type": "string"],
///                         "result": ["type": "number"]
///                     ],
///                     "required": ["explanation", "result"]
///                 ]
///             ],
///             "final_answer": ["type": "number"]
///         ],
///         "required": ["steps", "final_answer"]
///     ],
///     strict: true
/// )
/// ```
public struct JSONSchema: Codable, Sendable {
    /// The name of the schema.
    ///
    /// Used for identification and error messages.
    public let name: String
    
    /// The JSON Schema definition.
    ///
    /// Must be a valid JSON Schema that defines the structure of the response.
    public let schema: JSONValue
    
    /// Whether to enforce strict schema validation.
    ///
    /// When true, the model must exactly match the schema.
    public let strict: Bool?
    
    /// Creates a schema with pre-encoded JSON Schema.
    ///
    /// - Parameters:
    ///   - name: Schema identifier
    ///   - schema: JSON Schema as ``JSONValue``
    ///   - strict: Enable strict validation
    public init(name: String, schema: JSONValue, strict: Bool? = nil) {
        self.name = name
        self.schema = schema
        self.strict = strict
    }
    
    /// Creates a schema from a dictionary.
    ///
    /// - Parameters:
    ///   - name: Schema identifier
    ///   - schema: JSON Schema as a dictionary
    ///   - strict: Enable strict validation
    public init(name: String, schema: [String: Any], strict: Bool? = nil) {
        self.name = name
        self.schema = Self.convertToJSONValue(schema)
        self.strict = strict
    }
    
    private static func convertToJSONValue(_ dict: [String: Any]) -> JSONValue {
        var result = [String: JSONValue]()
        for (key, value) in dict {
            if let string = value as? String {
                result[key] = .string(string)
            } else if let int = value as? Int {
                result[key] = .int(int)
            } else if let double = value as? Double {
                result[key] = .double(double)
            } else if let bool = value as? Bool {
                result[key] = .bool(bool)
            } else if let dict = value as? [String: Any] {
                result[key] = convertToJSONValue(dict)
            } else if let array = value as? [Any] {
                result[key] = convertToJSONValue(array)
            }
        }
        return .object(result)
    }
    
    private static func convertToJSONValue(_ array: [Any]) -> JSONValue {
        var result = [JSONValue]()
        for value in array {
            if let string = value as? String {
                result.append(.string(string))
            } else if let int = value as? Int {
                result.append(.int(int))
            } else if let double = value as? Double {
                result.append(.double(double))
            } else if let bool = value as? Bool {
                result.append(.bool(bool))
            } else if let dict = value as? [String: Any] {
                result.append(convertToJSONValue(dict))
            } else if let array = value as? [Any] {
                result.append(convertToJSONValue(array))
            }
        }
        return .array(result)
    }
}

/// Audio configuration for chat completions.
///
/// Enables audio input processing or audio output generation depending on
/// the model's capabilities.
///
/// ## Example
/// ```swift
/// let audioConfig = ChatAudio(
///     format: "mp3",
///     voice: "alloy"
/// )
/// ```
public struct ChatAudio: Codable, Sendable {
    /// The audio format.
    ///
    /// Common formats include "mp3", "opus", "aac", "flac", "wav", and "pcm".
    public let format: String?
    
    /// The voice to use for audio generation.
    ///
    /// Available voices depend on the model. Common options include
    /// "alloy", "echo", "fable", "onyx", "nova", and "shimmer".
    public let voice: String?
    
    /// Creates audio configuration.
    ///
    /// - Parameters:
    ///   - format: Audio format
    ///   - voice: Voice selection for generation
    public init(format: String? = nil, voice: String? = nil) {
        self.format = format
        self.voice = voice
    }
}

/// Prediction configuration for improved latency.
///
/// Allows the model to skip processing of predicted content, reducing
/// response time when the expected output is known.
public struct Prediction: Codable, Sendable {
    /// The type of prediction.
    public let type: String
    
    /// The predicted content.
    ///
    /// The model will attempt to match this content and skip processing if possible.
    public let content: String
    
    /// Creates prediction configuration.
    ///
    /// - Parameters:
    ///   - type: Prediction type
    ///   - content: Expected content
    public init(type: String, content: String) {
        self.type = type
        self.content = content
    }
}

/// Options for streaming responses.
///
/// Controls additional features available when streaming is enabled.
///
/// ## Example
/// ```swift
/// let streamOptions = StreamOptions(includeUsage: true)
/// ```
public struct StreamOptions: Codable, Sendable {
    /// Whether to include usage statistics in stream chunks.
    ///
    /// When true, the final chunk will include token usage information.
    public let includeUsage: Bool?
    
    /// Creates streaming options.
    ///
    /// - Parameter includeUsage: Include usage stats in final chunk
    public init(includeUsage: Bool? = nil) {
        self.includeUsage = includeUsage
    }
}

/// Web search integration options.
///
/// Allows the model to search the web for current information when enabled.
public struct WebSearchOptions: Codable, Sendable {
    /// Whether web search is enabled.
    ///
    /// When true, the model can search for up-to-date information online.
    public let enabled: Bool
    
    /// Creates web search options.
    ///
    /// - Parameter enabled: Enable web search capability
    public init(enabled: Bool) {
        self.enabled = enabled
    }
}

/// Log probability information for tokens.
///
/// Provides insight into the model's token selection process, showing
/// the probability distribution over possible tokens at each position.
public struct Logprobs: Codable, Sendable {
    /// Log probability data for each content token.
    ///
    /// Each entry corresponds to a token in the generated content.
    public let content: [LogprobContent]?
}

/// Log probability data for a single token.
///
/// Shows the selected token and alternative possibilities with their
/// respective probabilities.
public struct LogprobContent: Codable, Sendable {
    /// The selected token.
    public let token: String
    
    /// The log probability of the selected token.
    ///
    /// More negative values indicate lower probability.
    public let logprob: Double
    
    /// UTF-8 byte representation of the token.
    public let bytes: [Int]?
    
    /// Alternative tokens and their probabilities.
    ///
    /// Shows what other tokens the model considered at this position.
    public let topLogprobs: [TopLogprob]?
}

/// Alternative token with its probability.
///
/// Represents a token that was considered but not selected by the model.
public struct TopLogprob: Codable, Sendable {
    /// The alternative token.
    public let token: String
    
    /// The log probability of this token.
    ///
    /// More negative values indicate lower probability.
    public let logprob: Double
    
    /// UTF-8 byte representation of the token.
    public let bytes: [Int]?
}